{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning for Sequential (Text) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxkleinegger/Desktop/test/speml.nosync/federated-text-classification/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-31 15:06:15,411\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import torch\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import reuters\n",
    "from typing import Dict\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from flwr.common import NDArrays, Scalar\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset hyperparameters\n",
    "MAX_LEN = 60\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 100\n",
    "NUM_CLIENTS = 10\n",
    "\n",
    "# General hyperparameters\n",
    "RANDOM_STATE = 42\n",
    "VERBOSE = True\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersDataset(Dataset):\n",
    "    def __init__(self, texts: list[list[int]], labels: list[int], maxlen: int):\n",
    "        self.texts = [text[:maxlen] for text in texts]\n",
    "        self.labels = labels\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = torch.tensor(self.texts[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate_fn(batch: list[tuple[list[int], list[int]]], vocab: Vocabulary):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts_padded, labels\n",
    "\n",
    "\n",
    "def _remove_stopwords(text: list[str]) -> pd.Series:\n",
    "    stop = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    return (\n",
    "        pd.Series(text)\n",
    "        .str.lower()\n",
    "        .replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "        .apply(nltk.word_tokenize)\n",
    "        .apply(\n",
    "            lambda sentence: \" \".join([word for word in sentence if word not in stop])\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def _process_labels(labels: list[list[str]]) -> pd.Series:\n",
    "    return pd.Series(labels).apply(lambda x: x[0] if x else \"unknown\")\n",
    "\n",
    "\n",
    "def load_dataset(maxlen: int):\n",
    "    nltk.download(\"reuters\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    # Load documents and their categories\n",
    "    documents = reuters.fileids()\n",
    "    categories = [reuters.categories(fileid) for fileid in documents]\n",
    "\n",
    "    # Load document content\n",
    "    data = [reuters.raw(fileid) for fileid in documents]\n",
    "\n",
    "    text = _remove_stopwords(data)\n",
    "    tokens = [nltk.word_tokenize(sentence) for sentence in text]\n",
    "\n",
    "    # Flatten the list of tokens for vocabulary creation\n",
    "    flat_tokens = [token for sentence in tokens for token in sentence]\n",
    "    vocab = Vocabulary(flat_tokens)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(_process_labels(categories))\n",
    "\n",
    "    # Add a padding token to the vocabulary\n",
    "    vocab.update([\"<pad>\"])\n",
    "    encoded_texts = [[vocab[token] for token in sentence] for sentence in tokens]\n",
    "\n",
    "    # Apply maxlen to encoded texts\n",
    "    encoded_texts = [text[:maxlen] for text in encoded_texts]\n",
    "\n",
    "    encoded_df = pd.DataFrame(\n",
    "        {\n",
    "            \"text\": encoded_texts,\n",
    "            \"category\": labels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return encoded_df, vocab, label_encoder\n",
    "\n",
    "\n",
    "def create_dataloader(text: pd.Series, labels: pd.Series, vocab: Vocabulary, maxlen: int,shuffle: bool = True) -> DataLoader:\n",
    "    dataset = ReutersDataset(text.tolist(), labels.tolist(), maxlen)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda x: _collate_fn(x, vocab),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/maxkleinegger/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maxkleinegger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maxkleinegger/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df, vocab, label_encoder = load_dataset(MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"], df[\"category\"], test_size=0.2, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(train_texts, train_labels, vocab, MAX_LEN)\n",
    "test_loader = create_dataloader(test_texts, test_labels, vocab, MAX_LEN, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the art baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, output_size: int, padding_idx: int, embed_size=128, hidden_size=256, num_layers=2, dropout=0.5):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        out = self.dropout(hidden)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, num_epochs: int, optimizer: optim.Optimizer, device: torch.device = DEVICE, verbose=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, device = DEVICE, verbose=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    total_loss /= len(test_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=1)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Test Loss: {total_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return total_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_centralised(model, train_loader: DataLoader, test_loader: DataLoader, epochs: int, lr: float, momentum: float = 0.9, device: torch.device = DEVICE, verbose=False):\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    train_model(model, train_loader, epochs, optim, device)\n",
    "    loss, accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BidirectionalLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    output_size= len(label_encoder.classes_),\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5051, Loss: 2.0296\n",
      "Precision: 0.7840, Recall: 0.5051, F1 Score: 0.4080\n"
     ]
    }
   ],
   "source": [
    "acc_baseline, _, _, _, _ = run_centralised(model, train_loader, test_loader, epochs=NUM_EPOCHS, lr=LEARNING_RATE, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/results_baseline.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.32s/it]\n",
      "100%|██████████| 1/1 [00:35<00:00, 35.88s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = BidirectionalLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    output_size= len(label_encoder.classes_),\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "epochs_count = 0\n",
    "for epochs in [1, 1, 3, 5, 15, 25, 50]:\n",
    "    epochs_count += epochs\n",
    "    acc_no_fed, loss_no_fed, precision, recall, f1 = run_centralised(model, train_loader, test_loader, epochs=epochs, lr=LEARNING_RATE, verbose=False)\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(f\"{epochs_count},{acc_no_fed},{loss_no_fed},{precision},{recall},{f1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM to be federated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        output_size: int,\n",
    "        padding_idx: int,\n",
    "        embed_size=128,\n",
    "        hidden_size=256,\n",
    "        num_layers=2,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        out = self.dropout(hidden)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    output_size= len(label_encoder.classes_),\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_baseline, _, _, _, _ = run_centralised(model, train_loader, test_loader, epochs=NUM_EPOCHS, lr=LEARNING_RATE, verbose=VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"results/results_baseline.csv\"\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    output_size= len(label_encoder.classes_),\n",
    "    padding_idx=vocab[\"<pad>\"],\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "epochs_count = 0\n",
    "for epochs in [1, 1, 3, 5, 15, 25, 50]:\n",
    "    epochs_count += epochs\n",
    "    acc_no_fed, loss_no_fed, precision, recall, f1 = run_centralised(model, train_loader, test_loader, epochs=epochs, lr=LEARNING_RATE, verbose=False)\n",
    "    with open(file_path, \"a\") as file:\n",
    "        file.write(f\"{epochs_count},{acc_no_fed},{loss_no_fed},{precision},{recall},{f1}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(train_loader, test_loader, vocab: Vocabulary, num_partitions: int, batch_size: int, val_ratio: float = 0.1,):\n",
    "    # Extract dataset from DataLoader\n",
    "    train_dataset = train_loader.dataset\n",
    "    test_dataset = test_loader.dataset\n",
    "\n",
    "    # Calculate partition lengths\n",
    "    num_images = len(train_dataset)\n",
    "    partition_len = [num_images // num_partitions] * num_partitions\n",
    "    partition_len[-1] += (\n",
    "        num_images % num_partitions\n",
    "    )  # Add remainder to the last partition\n",
    "\n",
    "    trainsets = random_split(\n",
    "        train_dataset, partition_len, torch.Generator().manual_seed(2023)\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders with train+val support\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for trainset in trainsets:\n",
    "        num_total = len(trainset)\n",
    "        num_val = int(val_ratio * num_total)\n",
    "        num_train = num_total - num_val\n",
    "\n",
    "        train_subset, val_subset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(2023))\n",
    "\n",
    "        trainloaders.append(DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=lambda x: _collate_fn(x, vocab)))\n",
    "        valloaders.append(DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=lambda x: _collate_fn(x, vocab)))\n",
    "\n",
    "    # Create DataLoader for the test set\n",
    "    testloader = DataLoader(test_dataset, batch_size=128, collate_fn=lambda x: _collate_fn(x, vocab))\n",
    "\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloaders, valloaders, testloader = partition_dataset(train_loader, test_loader, vocab, NUM_CLIENTS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# first partition\n",
    "train_partition = trainloaders[1].dataset\n",
    "\n",
    "# count data points\n",
    "partition_indices = train_partition.indices\n",
    "print(f\"number of articles: {len(partition_indices)}\")\n",
    "\n",
    "# visualise histogram\n",
    "plt.hist(\n",
    "    [train_partition.dataset[idx][1].item() for idx in partition_indices], bins=4\n",
    ")  # 46 classes in Reuters dataset\n",
    "plt.grid()\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Number of articles\")\n",
    "plt.title(\"Class labels distribution for Reuters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersClient(fl.client.NumPyClient):\n",
    "    def __init__(self, vocab_size, output_size, padding_idx, embed_size, hidden_size, num_layers, dropout, device, trainloader, valloader) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.model = TextRNN(\n",
    "            vocab_size=vocab_size,\n",
    "            output_size= output_size,\n",
    "            padding_idx=padding_idx,\n",
    "            embed_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        self.device = device\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]):\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        optim = torch.optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9)\n",
    "        train_model(model=self.model, train_loader=self.trainloader, num_epochs=1, optimizer=optim, device=self.device, verbose=False,)\n",
    "        return self.get_parameters({}), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy = evaluate_model(self.model, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn(vocab, output_size, testloader):\n",
    "    \"\"\"This is a function that returns a function. The returned\n",
    "    function (i.e. `evaluate_fn`) will be executed by the strategy\n",
    "    at the end of each round to evaluate the stat of the global\n",
    "    model.\"\"\"\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        \"\"\"This function is executed by the strategy it will instantiate\n",
    "        a model and replace its parameters with those from the global model.\n",
    "        The, the model will be evaluate on the test set (recall this is the\n",
    "        whole MNIST test set).\"\"\"\n",
    "\n",
    "        model = TextRNN(\n",
    "            vocab_size=len(vocab),\n",
    "            output_size=output_size,\n",
    "            padding_idx=vocab[\"<pad>\"],\n",
    "        )\n",
    "        # set parameters to the model\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        # call test\n",
    "        loss, accuracy = evaluate_model(\n",
    "            model, testloader\n",
    "        )  # <-------------------------- calls the `test` function, just what we did in the centralised setting\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate_fn\n",
    "\n",
    "\n",
    "def generate_client_fn(vocab, output_size, trainloaders, valloaders, device):\n",
    "    def client_fn(cid: str):\n",
    "        \"\"\"Returns a FlowerClient containing the cid-th data partition\"\"\"\n",
    "\n",
    "        return FlowerClient(\n",
    "            trainloader=trainloaders[int(cid)], valloader=valloaders[int(cid)]\n",
    "        ).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn(vocab_size, output_size, padding_idx, embed_size, hidden_size, num_layers, dropout, device, trainloaders, valloaders):\n",
    "    def client_fn(cid: str):\n",
    "        return ReutersClient(vocab_size, output_size, padding_idx, embed_size, hidden_size, num_layers, dropout, device, trainloader=trainloaders[int(cid)], valloader=valloaders[int(cid)]).to_client()\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = fl.server.strategy.FedAvg(\n",
    "            fraction_fit=1, \n",
    "            fraction_evaluate=1,  \n",
    "            min_available_clients=10, \n",
    "            evaluate_fn=get_evaluate_fn(vocab,  len(label_encoder.classes_), testloader),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1, \n",
    "    fraction_evaluate=1,  \n",
    "    min_available_clients=NUM_CLIENTS, \n",
    "    evaluate_fn=get_evaluate_fn(len(vocab), len(label_encoder.classes_), vocab[\"<pad>\"], EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, DEVICE, testloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_fn_callback = generate_client_fn(len(vocab), len(label_encoder.classes_), vocab[\"<pad>\"], EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT, DEVICE, trainloaders, valloaders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn_callback,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUM_EPOCHS * 10),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `history.metrics_centralized` contains the data as described\n",
    "print(f\"{history.metrics_centralized = }\")\n",
    "\n",
    "global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
    "rounds = [data[0] for data in global_accuracy_centralised]\n",
    "acc = [100.0 * data[1] for data in global_accuracy_centralised]\n",
    "\n",
    "plt.plot(rounds, acc, label='Accuracy')\n",
    "plt.axhline(y=70.5, color='r', linestyle='--', label='Centralized')\n",
    "plt.grid()\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.title(\"MNIST - IID - 100 clients with 10 clients per round\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
